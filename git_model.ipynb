{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7a5ea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:31:19.030798Z",
     "start_time": "2023-10-27T00:31:19.025332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:513: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\l\\Desktop\\final_embed1.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/l/Desktop/final_embed1.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/l/Desktop/final_embed1.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/l/Desktop/final_embed1.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/l/Desktop/final_embed1.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/l/Desktop/final_embed1.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m losses\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_variables \u001b[39mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minput_layer\u001b[39;00m \u001b[39mimport\u001b[39;00m Input\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\keras\\src\\models\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mimport\u001b[39;00m Functional\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequential\u001b[39;00m \u001b[39mimport\u001b[39;00m Sequential\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mengine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\keras\\src\\engine\\functional.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdtensor\u001b[39;00m \u001b[39mimport\u001b[39;00m layout_map \u001b[39mas\u001b[39;00m layout_map_lib\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_six\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     44\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\__init__.py:45\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[0;32m     42\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[0;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[0;32m     24\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m INFINITE \u001b[39mas\u001b[39;00m INFINITE_CARDINALITY\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:96\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[0;32m     95\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[0;32m     97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserver_lib\u001b[39;00m \u001b[39mimport\u001b[39;00m DispatchServer\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserver_lib\u001b[39;00m \u001b[39mimport\u001b[39;00m WorkerServer\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m compression_ops\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute_options\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoShardPolicy\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute_options\u001b[39;00m \u001b[39mimport\u001b[39;00m ExternalStatePolicy\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m structure\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[39mas\u001b[39;00m ged_ops\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwrapt\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m nest\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m composite_tensor\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msix\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_six\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n\u001b[1;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m sparse_tensor \u001b[39mas\u001b[39;00m _sparse_tensor\n\u001b[0;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m collections_abc \u001b[39mas\u001b[39;00m _collections_abc\n\u001b[0;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sorted\u001b[39m(dict_):\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m composite_tensor\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m constant_op\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m types_pb2\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m execute\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_callbacks\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m pywrap_tfe\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m core\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m dtypes\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:513\u001b[0m\n\u001b[0;32m    482\u001b[0m     _NP_TO_TF[pdt] \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\n\u001b[0;32m    483\u001b[0m         _NP_TO_TF[dt] \u001b[39mfor\u001b[39;00m dt \u001b[39min\u001b[39;00m _NP_TO_TF \u001b[39mif\u001b[39;00m dt \u001b[39m==\u001b[39m pdt()\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    486\u001b[0m TF_VALUE_DTYPES \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(_NP_TO_TF\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m    489\u001b[0m _TF_TO_NP \u001b[39m=\u001b[39m {\n\u001b[0;32m    490\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_HALF:\n\u001b[0;32m    491\u001b[0m         np\u001b[39m.\u001b[39mfloat16,\n\u001b[0;32m    492\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_FLOAT:\n\u001b[0;32m    493\u001b[0m         np\u001b[39m.\u001b[39mfloat32,\n\u001b[0;32m    494\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_DOUBLE:\n\u001b[0;32m    495\u001b[0m         np\u001b[39m.\u001b[39mfloat64,\n\u001b[0;32m    496\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT32:\n\u001b[0;32m    497\u001b[0m         np\u001b[39m.\u001b[39mint32,\n\u001b[0;32m    498\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT8:\n\u001b[0;32m    499\u001b[0m         np\u001b[39m.\u001b[39muint8,\n\u001b[0;32m    500\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT16:\n\u001b[0;32m    501\u001b[0m         np\u001b[39m.\u001b[39muint16,\n\u001b[0;32m    502\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT32:\n\u001b[0;32m    503\u001b[0m         np\u001b[39m.\u001b[39muint32,\n\u001b[0;32m    504\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT64:\n\u001b[0;32m    505\u001b[0m         np\u001b[39m.\u001b[39muint64,\n\u001b[0;32m    506\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT16:\n\u001b[0;32m    507\u001b[0m         np\u001b[39m.\u001b[39mint16,\n\u001b[0;32m    508\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT8:\n\u001b[0;32m    509\u001b[0m         np\u001b[39m.\u001b[39mint8,\n\u001b[0;32m    510\u001b[0m     \u001b[39m# NOTE(touts): For strings we use np.object as it supports variable length\u001b[39;00m\n\u001b[0;32m    511\u001b[0m     \u001b[39m# strings.\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_STRING:\n\u001b[1;32m--> 513\u001b[0m         np\u001b[39m.\u001b[39;49mobject,\n\u001b[0;32m    514\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_COMPLEX64:\n\u001b[0;32m    515\u001b[0m         np\u001b[39m.\u001b[39mcomplex64,\n\u001b[0;32m    516\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_COMPLEX128:\n\u001b[0;32m    517\u001b[0m         np\u001b[39m.\u001b[39mcomplex128,\n\u001b[0;32m    518\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT64:\n\u001b[0;32m    519\u001b[0m         np\u001b[39m.\u001b[39mint64,\n\u001b[0;32m    520\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_BOOL:\n\u001b[0;32m    521\u001b[0m         np\u001b[39m.\u001b[39mbool,\n\u001b[0;32m    522\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QINT8:\n\u001b[0;32m    523\u001b[0m         _np_qint8,\n\u001b[0;32m    524\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QUINT8:\n\u001b[0;32m    525\u001b[0m         _np_quint8,\n\u001b[0;32m    526\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QINT16:\n\u001b[0;32m    527\u001b[0m         _np_qint16,\n\u001b[0;32m    528\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QUINT16:\n\u001b[0;32m    529\u001b[0m         _np_quint16,\n\u001b[0;32m    530\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QINT32:\n\u001b[0;32m    531\u001b[0m         _np_qint32,\n\u001b[0;32m    532\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_BFLOAT16:\n\u001b[0;32m    533\u001b[0m         _np_bfloat16,\n\u001b[0;32m    534\u001b[0m \n\u001b[0;32m    535\u001b[0m     \u001b[39m# Ref types\u001b[39;00m\n\u001b[0;32m    536\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_HALF_REF:\n\u001b[0;32m    537\u001b[0m         np\u001b[39m.\u001b[39mfloat16,\n\u001b[0;32m    538\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_FLOAT_REF:\n\u001b[0;32m    539\u001b[0m         np\u001b[39m.\u001b[39mfloat32,\n\u001b[0;32m    540\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_DOUBLE_REF:\n\u001b[0;32m    541\u001b[0m         np\u001b[39m.\u001b[39mfloat64,\n\u001b[0;32m    542\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT32_REF:\n\u001b[0;32m    543\u001b[0m         np\u001b[39m.\u001b[39mint32,\n\u001b[0;32m    544\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT32_REF:\n\u001b[0;32m    545\u001b[0m         np\u001b[39m.\u001b[39muint32,\n\u001b[0;32m    546\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT8_REF:\n\u001b[0;32m    547\u001b[0m         np\u001b[39m.\u001b[39muint8,\n\u001b[0;32m    548\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT16_REF:\n\u001b[0;32m    549\u001b[0m         np\u001b[39m.\u001b[39muint16,\n\u001b[0;32m    550\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT16_REF:\n\u001b[0;32m    551\u001b[0m         np\u001b[39m.\u001b[39mint16,\n\u001b[0;32m    552\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT8_REF:\n\u001b[0;32m    553\u001b[0m         np\u001b[39m.\u001b[39mint8,\n\u001b[0;32m    554\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_STRING_REF:\n\u001b[0;32m    555\u001b[0m         np\u001b[39m.\u001b[39mobject,\n\u001b[0;32m    556\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_COMPLEX64_REF:\n\u001b[0;32m    557\u001b[0m         np\u001b[39m.\u001b[39mcomplex64,\n\u001b[0;32m    558\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_COMPLEX128_REF:\n\u001b[0;32m    559\u001b[0m         np\u001b[39m.\u001b[39mcomplex128,\n\u001b[0;32m    560\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_INT64_REF:\n\u001b[0;32m    561\u001b[0m         np\u001b[39m.\u001b[39mint64,\n\u001b[0;32m    562\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_UINT64_REF:\n\u001b[0;32m    563\u001b[0m         np\u001b[39m.\u001b[39muint64,\n\u001b[0;32m    564\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_BOOL_REF:\n\u001b[0;32m    565\u001b[0m         np\u001b[39m.\u001b[39mbool,\n\u001b[0;32m    566\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QINT8_REF:\n\u001b[0;32m    567\u001b[0m         _np_qint8,\n\u001b[0;32m    568\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QUINT8_REF:\n\u001b[0;32m    569\u001b[0m         _np_quint8,\n\u001b[0;32m    570\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QINT16_REF:\n\u001b[0;32m    571\u001b[0m         _np_qint16,\n\u001b[0;32m    572\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QUINT16_REF:\n\u001b[0;32m    573\u001b[0m         _np_quint16,\n\u001b[0;32m    574\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_QINT32_REF:\n\u001b[0;32m    575\u001b[0m         _np_qint32,\n\u001b[0;32m    576\u001b[0m     types_pb2\u001b[39m.\u001b[39mDT_BFLOAT16_REF:\n\u001b[0;32m    577\u001b[0m         _np_bfloat16,\n\u001b[0;32m    578\u001b[0m }\n\u001b[0;32m    580\u001b[0m _QUANTIZED_DTYPES_NO_REF \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m([qint8, quint8, qint16, quint16, qint32])\n\u001b[0;32m    581\u001b[0m _QUANTIZED_DTYPES_REF \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(\n\u001b[0;32m    582\u001b[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001b[1;32mc:\\Users\\l\\anaconda3\\envs\\koo\\lib\\site-packages\\numpy\\__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    300\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    301\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    302\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    307\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import ast\n",
    "from ast import literal_eval\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a6b1866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:31:22.249692Z",
     "start_time": "2023-10-27T00:31:22.243907Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_clf_eval(y_test, pred):\n",
    "    score = []\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "    specificity = tn/(tn + fp)\n",
    "    roc_auc = roc_auc_score(y_test, pred)\n",
    "\n",
    "    score = [accuracy, precision, recall, f1, specificity, roc_auc]\n",
    "    return score\n",
    "def get_mean(accuracy, precision, recall, f1,specificity,roc_auc):\n",
    "  print(\"accuracy: \")\n",
    "  print(np.mean(accuracy))\n",
    "  print(np.std(accuracy))\n",
    "  print(\"precision: \")\n",
    "  print(np.mean(precision))\n",
    "  print(np.std(precision))\n",
    "  print(\"recall: \")\n",
    "  print(np.mean(recall))\n",
    "  print(np.std(recall))\n",
    "  print(\"f1: \")\n",
    "  print(np.mean(f1))\n",
    "  print(np.std(f1))\n",
    "  print(\"specificity: \")\n",
    "  print(np.mean(specificity))\n",
    "  print(np.std(specificity))\n",
    "  print(\"roc_auc: \")\n",
    "  print(np.mean(roc_auc))\n",
    "  print(np.std(roc_auc))\n",
    "  return print(\"end\")\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "384bd809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:28:58.976980Z",
     "start_time": "2023-10-27T00:28:54.019345Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_use.csv', encoding = 'utf-8-sig', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bba7014",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:29:06.872582Z",
     "start_time": "2023-10-27T00:28:59.201293Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: ast\u001b[38;5;241m.\u001b[39mliteral_eval(x))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4760\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4627\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4632\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4633\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4634\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4635\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4636\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4751\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4754\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4755\u001b[0m         func,\n\u001b[0;32m   4756\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4757\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4758\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4759\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4760\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1207\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1287\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1281\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1288\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1289\u001b[0m )\n\u001b[0;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2917\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessing\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: ast\u001b[38;5;241m.\u001b[39mliteral_eval(x))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ast.py:110\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert(node_or_string)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ast.py:84\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_num(node)\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert\u001b[39m(node):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant):\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['preprocessing'] = df['preprocessing'].apply(lambda x: ast.literal_eval(x))\n",
    "#text_num = 100\n",
    "#df['preprocessing'] = df['preprocessing'].apply(lambda x: to_string(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fa443",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:29:06.875575Z",
     "start_time": "2023-10-27T00:29:06.875575Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = 48972\n",
    "text_num = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b217812",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T00:29:06.878567Z",
     "start_time": "2023-10-27T00:29:06.878567Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(df, vocab_size, text_num):\n",
    "    X = df[['rating', 'preprocessing', 'depth',\n",
    "       'text_structure', 'review_extremity', 'reputation', 'experience',\n",
    "       'location', 'Psychological Distancing', 'style', 'photo',\n",
    "        'motion', 'Affect', 'Cognition', 'Perception', 'Social']] \n",
    "    y = df[['fake']]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 215, stratify=y)\n",
    "    \n",
    "    tokenizer = Tokenizer(vocab_size)\n",
    "    tokenizer.fit_on_texts(X_train['preprocessing'].astype(str))\n",
    "    X_train['text_encode'] = tokenizer.texts_to_sequences(X_train['preprocessing'].astype(str))\n",
    "#     length = [len(sample) for sample in X_train['text_encode']]\n",
    "#     length.sort()\n",
    "    #text_num = length[int(len(length)*0.9)]\n",
    "    train_text = pad_sequences(X_train['text_encode'].values.tolist(), maxlen=text_num)\n",
    "    \n",
    "    #linguistic feature\n",
    "    \n",
    "    train_affect = np.array(X_train['Affect'].values.tolist()).reshape((-1, 1))\n",
    "    train_motion = np.array(X_train['motion'].values.tolist()).reshape((-1, 1))\n",
    "    train_cognition = np.array(X_train['Cognition'].values.tolist()).reshape((-1, 1))\n",
    "    train_perception = np.array(X_train['Perception'].values.tolist()).reshape((-1, 1))\n",
    "    train_social = np.array(X_train['Social'].values.tolist()).reshape((-1, 1))\n",
    "    train_p_dis = np.array(X_train['Psychological Distancing'].values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    #review feature\n",
    "    \n",
    "    train_rating = np.array(X_train['rating'].values.tolist()).reshape((-1, 1))\n",
    "    train_depth = np.array(X_train['depth'].values.tolist()).reshape((-1, 1))\n",
    "    train_structure = np.array(X_train['text_structure'].values.tolist()).reshape((-1, 1))\n",
    "    train_extremity = np.array(X_train['review_extremity'].values.tolist()).reshape((-1, 1))\n",
    "    train_photo = np.array(X_train['photo'].values.tolist()).reshape((-1, 1))\n",
    "    train_style = np.array(X_train['style'].values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    #reviewer feature\n",
    "    \n",
    "    train_reputation = np.array(X_train['reputation'].values.tolist()).reshape((-1, 1))\n",
    "    train_experience = np.array(X_train['experience'].values.tolist()).reshape((-1, 1))\n",
    "    train_location = np.array(X_train['location'].values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    train_fake = np.array(y_train.values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    \n",
    "    #linguistic feature\n",
    "    \n",
    "    X_test['text_encode'] = tokenizer.texts_to_sequences(X_test['preprocessing'].astype(str))\n",
    "    test_text = pad_sequences(X_test['text_encode'].values.tolist(), maxlen=text_num)\n",
    "    test_affect = np.array(X_test['Affect'].values.tolist()).reshape((-1, 1))\n",
    "    test_motion = np.array(X_test['motion'].values.tolist()).reshape((-1, 1))\n",
    "    test_cognition = np.array(X_test['Cognition'].values.tolist()).reshape((-1, 1))\n",
    "    test_perception = np.array(X_test['Perception'].values.tolist()).reshape((-1, 1))\n",
    "    test_social = np.array(X_test['Social'].values.tolist()).reshape((-1, 1))\n",
    "    test_p_dis = np.array(X_test['Psychological Distancing'].values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    #review feature\n",
    "    \n",
    "    test_rating = np.array(X_test['rating'].values.tolist()).reshape((-1, 1))\n",
    "    test_depth = np.array(X_test['depth'].values.tolist()).reshape((-1, 1))\n",
    "    test_structure = np.array(X_test['text_structure'].values.tolist()).reshape((-1, 1))\n",
    "    test_extremity = np.array(X_test['review_extremity'].values.tolist()).reshape((-1, 1))\n",
    "    test_photo = np.array(X_test['photo'].values.tolist()).reshape((-1, 1))\n",
    "    test_style = np.array(X_test['style'].values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    #reviewer feature\n",
    "    \n",
    "    test_reputation = np.array(X_test['reputation'].values.tolist()).reshape((-1, 1))\n",
    "    test_experience = np.array(X_test['experience'].values.tolist()).reshape((-1, 1))\n",
    "    test_location = np.array(X_test['location'].values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    test_fake = np.array(y_test.values.tolist()).reshape((-1, 1))\n",
    "    \n",
    "    return train_text, train_affect, train_motion, train_cognition, train_perception, train_social, train_p_dis, train_rating, \\\n",
    "train_depth, train_structure, train_extremity, train_photo, train_style, train_reputation, train_experience, train_location, train_fake, \\\n",
    "test_text, test_affect, test_motion, test_cognition, test_perception, test_social, test_p_dis, test_rating, test_depth, test_structure, \\\n",
    "test_extremity, test_photo, test_style, test_reputation, test_experience, test_location, test_fake\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e90cf9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_text, train_affect, train_motion, train_cognition, train_perception, train_social, train_p_dis, train_rating, \\\n",
    "train_depth, train_structure, train_extremity, train_photo, train_style, train_reputation, train_experience, train_location, train_fake, \\\n",
    "test_text, test_affect, test_motion, test_cognition, test_perception, test_social, test_p_dis, test_rating, test_depth, test_structure, \\\n",
    "test_extremity, test_photo, test_style, test_reputation, test_experience, test_location, test_fake= preprocess(df, vocab_size, text_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23beae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = X_train['preprocessing']\n",
    "test_text = X_test['preprocessing']\n",
    "\n",
    "#CBOW\n",
    "sentences = [sentence.split() for sentence in train_text]\n",
    "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "train_text = tokenizer.texts_to_sequences(train_text)\n",
    "test_text = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100\n",
    "train_text = pad_sequences(train_text, maxlen=max_length)\n",
    "test_text = pad_sequences(test_text, maxlen=max_length)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "\n",
    "\n",
    "# Create a weight matrix for the embedding layer\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[i] = w2v_model.wv[word]\n",
    "embedding_layer = Embedding(vocab_size, 100, weights = [embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15960486",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = X_train['preprocessing']\n",
    "test_text = X_test['preprocessing']\n",
    "\n",
    "#glove\n",
    "sentences = [sentence.split() for sentence in train_text]\n",
    "#w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window=5)\n",
    "glove = Glove(no_components=100, learning_rate=0.0001)\n",
    "\n",
    "# 학습에 이용할 쓰레드의 개수는 4로 설정, 에포크는 20.\n",
    "glove.fit(corpus.matrix, epochs=50, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "word_dict = {}\n",
    "for word in  glove.dictionary.keys():\n",
    "    word_dict[word] = glove.word_vectors[glove.dictionary[word]]\n",
    "print('[Success !] Lengh of word dict... : ', len(word_dict))\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "train_text = tokenizer.texts_to_sequences(train_text)\n",
    "test_text = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_length = 100\n",
    "train_text = pad_sequences(train_text, maxlen=max_length)\n",
    "test_text = pad_sequences(test_text, maxlen=max_length)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word_dict.keys():\n",
    "    \tembedding_matrix[i] = word_dict.get(word, np.zeros(100))\n",
    "     \n",
    "embedding_layer = Embedding(vocab_size, 100, weights = [embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75035f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN 있는 거1\n",
    "def Model_Build(text_num, vocab_size):\n",
    "    \n",
    "    tf.random.set_seed(215)\n",
    "    \n",
    "    #CNN\n",
    "    text_input = Input(shape=(text_num, ))\n",
    "    text_embedding = Embedding(vocab_size, 100, input_length = text_input.shape[1], name=\"text-embedding\")(text_input)\n",
    "\n",
    "#    text_embedding = embedding_layer(text_input)\n",
    "    text_dropout = Dropout(0.2)(text_embedding)\n",
    "    text_conv = Conv1D(256, 3, padding='valid', activation='relu')(text_dropout)\n",
    "    text_pool = GlobalMaxPooling1D()(text_conv)\n",
    "    text_pool = Dense(units = 128, activation='relu')(text_pool)\n",
    "    \n",
    "    #linguistic\n",
    "    affect_input = Input(shape = (1,))\n",
    "    dense_affect = Dense(units=64, activation='relu', name='dense_affect')(affect_input)\n",
    "    motion_input = Input(shape = (1,))\n",
    "    dense_motion = Dense(units=64, activation='relu', name='motion')(motion_input)\n",
    "    cognitive_input = Input(shape = (1,))\n",
    "    dense_cognitive = Dense(units=64, activation='relu', name='dense_cognitive')(cognitive_input)\n",
    "    perception_input = Input(shape = (1,))\n",
    "    dense_perception = Dense(units=64, activation='relu', name='dense_perception')(perception_input)\n",
    "    social_input = Input(shape = (1,))\n",
    "    dense_social = Dense(units=64, activation='relu', name='dense_social')(social_input)\n",
    "    p_dis_input = Input(shape = (1,))\n",
    "    dense_p_dis = Dense(units=64, activation='relu', name='dense_p_dis')(p_dis_input)\n",
    "\n",
    "    linguistic_feature = Concatenate(name = 'Linguistic_Feature_Layer')([dense_affect, dense_motion, dense_cognitive, dense_perception, dense_social, dense_p_dis])\n",
    "    linguistic_feature1 = Dense(units=128, activation='relu', name='dense_linguistic')(linguistic_feature)\n",
    "\n",
    "    #review\n",
    "    rating_input = Input(shape = (1,))\n",
    "    dense_rating = Dense(units=64, activation='relu', name='dense_rating')(rating_input)\n",
    "    depth_input = Input(shape = (1,))\n",
    "    dense_depth = Dense(units=64, activation='relu', name='dense_depth')(depth_input)\n",
    "    structure_input = Input(shape = (1,))\n",
    "    dense_structure = Dense(units=64, activation='relu', name='dense_structure')(structure_input)\n",
    "    extremity_input = Input(shape = (1,))\n",
    "    dense_extremity = Dense(units=64, activation='relu', name='dense_extremity')(extremity_input)\n",
    "    photo_input = Input(shape = (1,))\n",
    "    dense_photo = Dense(units=64, activation='relu', name='dense_photo')(photo_input)\n",
    "    style_input = Input(shape = (1,))\n",
    "    dense_style = Dense(units=64, activation='relu', name='dense_style')(style_input)\n",
    "\n",
    "    review_feature = Concatenate(name = 'review_Feature_Layer')([dense_rating, dense_depth, dense_structure, dense_extremity, dense_photo, dense_style])\n",
    "    review_feature1 = Dense(units=128, activation='relu', name='dense_review')(review_feature)\n",
    "\n",
    "    #reviewer\n",
    "    \n",
    "    reputation_input = Input(shape = (1,))\n",
    "    dense_reputation = Dense(units=64, activation='relu', name='dense_reputation')(reputation_input)\n",
    "    experience_input = Input(shape = (1,))\n",
    "    dense_experience = Dense(units=64, activation='relu', name='dense_experience')(experience_input)\n",
    "    location_input = Input(shape = (1,))\n",
    "    dense_location = Dense(units=64, activation='relu', name='dense_location')(location_input)\n",
    "    \n",
    "    reviewer_feature = Concatenate(name = 'reviewer_Feature_Layer')([dense_reputation, dense_location, dense_experience])\n",
    "    reviewer_feature1 = Dense(units=128, activation='relu', name='dense_reviewer')(reviewer_feature)\n",
    "\n",
    "    all_feature = Concatenate(name = 'All_Feature_Layer')([text_pool, reviewer_feature1, review_feature1, linguistic_feature1])\n",
    "    #all_feature1 = Dense(units=256, activation='relu', name='dense_all')(all_feature)\n",
    "    #all_feature2 = Dense(units=128, activation='relu', name='dense_all1')(all_feature)\n",
    "    all_feature3 = Dense(units=64, activation='relu', name='dense_all2')(all_feature)\n",
    "    all_feature4 = Dense(units=32, activation='relu', name='dense_all3')(all_feature3)\n",
    "    all_feature5 = Dense(units=16, activation='relu', name='dense_all4')(all_feature4)\n",
    "\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name = 'outputs')(all_feature5)  # (1,1) / h(8,1)초기화\n",
    "    model = Model(inputs=[text_input, affect_input, motion_input, cognitive_input, perception_input,social_input, p_dis_input, rating_input, depth_input, structure_input, extremity_input, photo_input, style_input, reputation_input, experience_input, location_input], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a5bba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "142/142 [==============================] - 65s 449ms/step - loss: 0.3541 - val_loss: 0.2342\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 66s 464ms/step - loss: 0.2252 - val_loss: 0.2213\n",
      "Epoch 3/100\n",
      " 18/142 [==>...........................] - ETA: 55s - loss: 0.2158"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\AIBIZSPACE21\\ETC\\Desktop\\koo\\final.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39madam, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m es \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, patience \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit([train_text, train_affect, train_motion, train_cognition, train_perception, train_social, train_p_dis, train_rating, train_depth, train_structure, train_extremity, train_photo, train_style, train_reputation, train_experience, train_location], \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             train_fake, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             batch_size \u001b[39m=\u001b[39;49m \u001b[39m2048\u001b[39;49m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             callbacks\u001b[39m=\u001b[39;49m[es],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m             validation_split \u001b[39m=\u001b[39;49m \u001b[39m0.2\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m prediction4 \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict([test_text, test_affect, test_motion, test_cognition, test_perception, test_social, test_p_dis, test_rating, test_depth, test_structure, test_extremity, test_photo, test_style, test_reputation, test_experience, test_location])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/AIBIZSPACE21/ETC/Desktop/koo/final.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m pred_class4 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(prediction4 \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m , \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\AIBIZSPACE21\\anaconda3\\envs\\RC\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score4 = []\n",
    "acc = []\n",
    "precision = []\n",
    "recall=[]\n",
    "f1=[]\n",
    "spe = []\n",
    "roc = []\n",
    "for i in range(5):\n",
    "    model = Model_Build(text_num, vocab_size)\n",
    "\n",
    "    adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)\n",
    "\n",
    "\n",
    "    history = model.fit([train_text, train_affect, train_motion, train_cognition, train_perception, train_social, train_p_dis, train_rating, train_depth, train_structure, train_extremity, train_photo, train_style, train_reputation, train_experience, train_location], \n",
    "                train_fake, \n",
    "                batch_size = 2048, \n",
    "                epochs = 100,\n",
    "                callbacks=[es],\n",
    "                validation_split = 0.2)\n",
    "\n",
    "    prediction4 = model.predict([test_text, test_affect, test_motion, test_cognition, test_perception, test_social, test_p_dis, test_rating, test_depth, test_structure, test_extremity, test_photo, test_style, test_reputation, test_experience, test_location])\n",
    "\n",
    "    pred_class4 = np.where(prediction4 >= 0.5, 1 , 0)\n",
    "    score4 = get_clf_eval(test_fake, pred_class4)\n",
    "    acc.append(score4[0])\n",
    "    precision.append(score4[1])\n",
    "    recall.append(score4[2])\n",
    "    f1.append(score4[3])\n",
    "    spe.append(score4[4])\n",
    "    roc.append(score4[5])\n",
    "    print(score4)\n",
    "print(get_mean(acc, precision, recall, f1, spe, roc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edcde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN만\n",
    "def Model_Build(text_num, vocab_size):\n",
    "    \n",
    "    tf.random.set_seed(215)\n",
    "    \n",
    "    #CNN\n",
    "    text_input = Input(shape=(text_num, ))\n",
    "    text_embedding = Embedding(vocab_size, 100, input_length = text_input.shape[1], name=\"text-embedding\")(text_input)\n",
    "    text_dropout = Dropout(0.2)(text_embedding)\n",
    "    text_conv = Conv1D(256, 3, padding='valid', activation='relu')(text_dropout)\n",
    "    text_pool = GlobalMaxPooling1D()(text_conv)\n",
    "    text_pool = Dense(units = 128, activation='relu')(text_pool)\n",
    "    \n",
    "    #review\n",
    "    rating_input = Input(shape = (1,))\n",
    "    dense_rating = Dense(units=64, activation='relu', name='dense_rating')(rating_input)\n",
    "    depth_input = Input(shape = (1,))\n",
    "    dense_depth = Dense(units=64, activation='relu', name='dense_depth')(depth_input)\n",
    "    structure_input = Input(shape = (1,))\n",
    "    dense_structure = Dense(units=64, activation='relu', name='dense_structure')(structure_input)\n",
    "    extremity_input = Input(shape = (1,))\n",
    "    dense_extremity = Dense(units=64, activation='relu', name='dense_extremity')(extremity_input)\n",
    "    photo_input = Input(shape = (1,))\n",
    "    dense_photo = Dense(units=64, activation='relu', name='dense_photo')(photo_input)\n",
    "    style_input = Input(shape = (1,))\n",
    "    dense_style = Dense(units=64, activation='relu', name='dense_style')(style_input)\n",
    "\n",
    "    review_feature = Concatenate(name = 'review_Feature_Layer')([dense_rating, dense_depth, dense_structure, dense_extremity, dense_photo, dense_style])\n",
    "    review_feature1 = Dense(units=128, activation='relu', name='dense_review')(review_feature)\n",
    "\n",
    "    #reviewer\n",
    "    \n",
    "    reputation_input = Input(shape = (1,))\n",
    "    dense_reputation = Dense(units=64, activation='relu', name='dense_reputation')(reputation_input)\n",
    "    experience_input = Input(shape = (1,))\n",
    "    dense_experience = Dense(units=64, activation='relu', name='dense_experience')(experience_input)\n",
    "    location_input = Input(shape = (1,))\n",
    "    dense_location = Dense(units=64, activation='relu', name='dense_location')(location_input)\n",
    "    \n",
    "    reviewer_feature = Concatenate(name = 'reviewer_Feature_Layer')([dense_reputation, dense_location, dense_experience])\n",
    "    reviewer_feature1 = Dense(units=128, activation='relu', name='dense_reviewer')(reviewer_feature)\n",
    "\n",
    "    all_feature = Concatenate(name = 'All_Feature_Layer')([text_pool, reviewer_feature1, review_feature1])\n",
    "    #all_feature1 = Dense(units=256, activation='relu', name='dense_all')(all_feature)\n",
    "    #all_feature2 = Dense(units=128, activation='relu', name='dense_all1')(all_feature)\n",
    "    all_feature3 = Dense(units=64, activation='relu', name='dense_all2')(all_feature)\n",
    "    all_feature4 = Dense(units=32, activation='relu', name='dense_all3')(all_feature3)\n",
    "    all_feature5 = Dense(units=16, activation='relu', name='dense_all4')(all_feature4)\n",
    "\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name = 'outputs')(all_feature5)  # (1,1) / h(8,1)초기화\n",
    "    model = Model(inputs=[text_input, rating_input, depth_input, structure_input, extremity_input, photo_input, style_input, reputation_input, experience_input, location_input], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e8f23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1884 - val_loss: 0.2002\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1835 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1790 - val_loss: 0.1950\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1976\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1722 - val_loss: 0.1979\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1689 - val_loss: 0.2013\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1664 - val_loss: 0.2022\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1643 - val_loss: 0.2046\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1610 - val_loss: 0.2065\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1593 - val_loss: 0.2120\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1570 - val_loss: 0.2121\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1550 - val_loss: 0.2166\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1522 - val_loss: 0.2187\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.2005\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1960\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1966\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1969\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1690 - val_loss: 0.2003\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2019\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1649 - val_loss: 0.2046\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1611 - val_loss: 0.2062\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1594 - val_loss: 0.2120\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1571 - val_loss: 0.2124\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1545 - val_loss: 0.2173\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1521 - val_loss: 0.2192\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.2007\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1756 - val_loss: 0.1975\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1725 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1693 - val_loss: 0.2006\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1665 - val_loss: 0.2023\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2056\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1611 - val_loss: 0.2066\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1596 - val_loss: 0.2124\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1571 - val_loss: 0.2128\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2178\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1520 - val_loss: 0.2191\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 119ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2050 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.1994\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1836 - val_loss: 0.1960\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1757 - val_loss: 0.1965\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1973\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1690 - val_loss: 0.2008\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2025\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2050\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1610 - val_loss: 0.2062\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1596 - val_loss: 0.2116\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1572 - val_loss: 0.2119\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1546 - val_loss: 0.2179\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1525 - val_loss: 0.2188\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.2006\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1969\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1690 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2019\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1645 - val_loss: 0.2050\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1611 - val_loss: 0.2066\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1593 - val_loss: 0.2122\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1574 - val_loss: 0.2125\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1546 - val_loss: 0.2168\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1521 - val_loss: 0.2194\n",
      "Epoch 00018: early stopping\n",
      "accuracy: 0.9174964124075505, precision: 0.5957228400342173, recall: 0.4052135459094612, f1: 0.4823382739991689, specificity0.9711818725075307, roc_auc0.688197709208496\n",
      "accuracy: 0.9174964124075505, precision: 0.5957228400342173, recall: 0.4052135459094612, f1: 0.4823382739991689, specificity0.9711818725075307, roc_auc0.688197709208496\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 119ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2116\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2047 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1951 - val_loss: 0.1983\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1884 - val_loss: 0.2005\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1835 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1790 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1975\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1721 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1690 - val_loss: 0.2011\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1663 - val_loss: 0.2023\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1643 - val_loss: 0.2054\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1609 - val_loss: 0.2067\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1590 - val_loss: 0.2123\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1573 - val_loss: 0.2123\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1543 - val_loss: 0.2178\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1520 - val_loss: 0.2193\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.1994\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1836 - val_loss: 0.1959\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1954\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1756 - val_loss: 0.1966\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1723 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1691 - val_loss: 0.2034\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1663 - val_loss: 0.2027\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2049\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1608 - val_loss: 0.2071\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1591 - val_loss: 0.2124\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1570 - val_loss: 0.2125\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2172\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1523 - val_loss: 0.2188\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1884 - val_loss: 0.1976\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1831 - val_loss: 0.1957\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1955\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1761 - val_loss: 0.1966\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1970\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1693 - val_loss: 0.2011\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2024\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1643 - val_loss: 0.2052\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1608 - val_loss: 0.2069\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1593 - val_loss: 0.2127\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1569 - val_loss: 0.2131\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1543 - val_loss: 0.2181\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1519 - val_loss: 0.2187\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 123ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1952 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1884 - val_loss: 0.1994\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1835 - val_loss: 0.1960\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1969\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1723 - val_loss: 0.1968\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1691 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1665 - val_loss: 0.2030\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1641 - val_loss: 0.2052\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1608 - val_loss: 0.2070\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1589 - val_loss: 0.2125\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1569 - val_loss: 0.2132\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2180\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1519 - val_loss: 0.2187\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 119ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2007\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1959\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1972\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1722 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1689 - val_loss: 0.2009\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2018\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2055\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1611 - val_loss: 0.2066\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1594 - val_loss: 0.2124\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1570 - val_loss: 0.2127\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1547 - val_loss: 0.2170\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1521 - val_loss: 0.2192\n",
      "Epoch 00018: early stopping\n",
      "accuracy: 0.9177944585495088, precision: 0.6014159292035398, recall: 0.39543814732922145, f1: 0.4771466685389314, specificity0.9725355805700209, roc_auc0.6839868639496212\n",
      "accuracy: 0.9177944585495088, precision: 0.6014159292035398, recall: 0.39543814732922145, f1: 0.4771466685389314, specificity0.9725355805700209, roc_auc0.6839868639496212\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2277 - val_loss: 0.2217\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.1989\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1835 - val_loss: 0.1970\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1980\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1723 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1693 - val_loss: 0.2041\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2025\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1646 - val_loss: 0.2051\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1608 - val_loss: 0.2070\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1591 - val_loss: 0.2123\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1569 - val_loss: 0.2126\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2173\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1520 - val_loss: 0.2182\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2006\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1836 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1954\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1973\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1723 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1692 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2018\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1645 - val_loss: 0.2049\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1611 - val_loss: 0.2065\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1595 - val_loss: 0.2113\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1573 - val_loss: 0.2117\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2174\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1521 - val_loss: 0.2191\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.1991\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1835 - val_loss: 0.1957\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1790 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1757 - val_loss: 0.1968\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1722 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1689 - val_loss: 0.2012\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1663 - val_loss: 0.2022\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1642 - val_loss: 0.2048\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1609 - val_loss: 0.2069\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1593 - val_loss: 0.2125\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1569 - val_loss: 0.2129\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1545 - val_loss: 0.2177\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1523 - val_loss: 0.2181\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1952 - val_loss: 0.1982\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1884 - val_loss: 0.2010\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1836 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1955\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1973\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1725 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1691 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2021\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1645 - val_loss: 0.2050\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1612 - val_loss: 0.2065\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1592 - val_loss: 0.2122\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1573 - val_loss: 0.2119\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1545 - val_loss: 0.2176\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1523 - val_loss: 0.2197\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2153 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2031\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1951 - val_loss: 0.1983\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1884 - val_loss: 0.1995\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1835 - val_loss: 0.1959\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1955\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1978\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1973\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1690 - val_loss: 0.2013\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2024\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1645 - val_loss: 0.2056\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1610 - val_loss: 0.2069\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1589 - val_loss: 0.2131\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1570 - val_loss: 0.2129\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2183\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1520 - val_loss: 0.2196\n",
      "Epoch 00018: early stopping\n",
      "accuracy: 0.9182249696434485, precision: 0.6089354660783233, recall: 0.38543000116373793, f1: 0.47206385404789053, specificity0.9740600265863385, roc_auc0.6797450138750383\n",
      "accuracy: 0.9182249696434485, precision: 0.6089354660783233, recall: 0.38543000116373793, f1: 0.47206385404789053, specificity0.9740600265863385, roc_auc0.6797450138750383\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2050 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.2008\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1961\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1954\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1757 - val_loss: 0.1971\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1725 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1692 - val_loss: 0.2009\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2025\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2053\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1610 - val_loss: 0.2065\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1594 - val_loss: 0.2117\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1572 - val_loss: 0.2117\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2177\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1519 - val_loss: 0.2187\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1952 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2011\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1963\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1790 - val_loss: 0.1955\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1977\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1725 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1692 - val_loss: 0.2009\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1665 - val_loss: 0.2019\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2052\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1612 - val_loss: 0.2065\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1589 - val_loss: 0.2124\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1571 - val_loss: 0.2125\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1547 - val_loss: 0.2174\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1522 - val_loss: 0.2190\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1952 - val_loss: 0.1983\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2007\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1836 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1790 - val_loss: 0.1951\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1978\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1725 - val_loss: 0.1973\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1689 - val_loss: 0.2013\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1664 - val_loss: 0.2022\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1645 - val_loss: 0.2051\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1610 - val_loss: 0.2070\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1594 - val_loss: 0.2120\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1571 - val_loss: 0.2125\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2175\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1521 - val_loss: 0.2193\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.1995\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1836 - val_loss: 0.1960\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1790 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1760 - val_loss: 0.1966\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1723 - val_loss: 0.1970\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1690 - val_loss: 0.2017\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1665 - val_loss: 0.2021\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1643 - val_loss: 0.2054\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1610 - val_loss: 0.2066\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1591 - val_loss: 0.2126\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1570 - val_loss: 0.2125\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2186\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1519 - val_loss: 0.2194\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2277 - val_loss: 0.2217\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2008\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1959\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1790 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1976\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1723 - val_loss: 0.1970\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1690 - val_loss: 0.2008\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1663 - val_loss: 0.2027\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1643 - val_loss: 0.2045\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1608 - val_loss: 0.2068\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1589 - val_loss: 0.2124\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1569 - val_loss: 0.2124\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1543 - val_loss: 0.2170\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1520 - val_loss: 0.2188\n",
      "Epoch 00018: early stopping\n",
      "accuracy: 0.9181918534054532, precision: 0.6063714902807775, recall: 0.39206330734318634, f1: 0.4762174005230052, specificity0.973328292498506, roc_auc0.6826957999208462\n",
      "accuracy: 0.9181918534054532, precision: 0.6063714902807775, recall: 0.39206330734318634, f1: 0.4762174005230052, specificity0.973328292498506, roc_auc0.6826957999208462\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2008\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1962\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1792 - val_loss: 0.1956\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1975\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1693 - val_loss: 0.2026\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1667 - val_loss: 0.2018\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1645 - val_loss: 0.2049\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1612 - val_loss: 0.2065\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1591 - val_loss: 0.2123\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1575 - val_loss: 0.2121\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1549 - val_loss: 0.2169\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1523 - val_loss: 0.2187\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2119\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1885 - val_loss: 0.2007\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1836 - val_loss: 0.1960\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1791 - val_loss: 0.1957\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1977\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1972\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1690 - val_loss: 0.2012\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1663 - val_loss: 0.2023\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1643 - val_loss: 0.2054\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1609 - val_loss: 0.2068\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1588 - val_loss: 0.2125\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1570 - val_loss: 0.2123\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1544 - val_loss: 0.2180\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1522 - val_loss: 0.2188\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2048 - val_loss: 0.2032\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1952 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1883 - val_loss: 0.2001\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1834 - val_loss: 0.1957\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1789 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1758 - val_loss: 0.1973\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1722 - val_loss: 0.1974\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1688 - val_loss: 0.2011\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1663 - val_loss: 0.2024\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1644 - val_loss: 0.2055\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1610 - val_loss: 0.2068\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1592 - val_loss: 0.2128\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1571 - val_loss: 0.2133\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1543 - val_loss: 0.2176\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1518 - val_loss: 0.2184\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 119ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2154 - val_loss: 0.2117\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2034\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1886 - val_loss: 0.2007\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1837 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1792 - val_loss: 0.1952\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1978\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1724 - val_loss: 0.1971\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1690 - val_loss: 0.2011\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1664 - val_loss: 0.2020\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1645 - val_loss: 0.2052\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1611 - val_loss: 0.2064\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1595 - val_loss: 0.2122\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1577 - val_loss: 0.2119\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1545 - val_loss: 0.2172\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 118ms/step - loss: 0.1522 - val_loss: 0.2187\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 18s 118ms/step - loss: 0.3538 - val_loss: 0.2375\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2278 - val_loss: 0.2218\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2153 - val_loss: 0.2118\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.2049 - val_loss: 0.2033\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1884 - val_loss: 0.2006\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1835 - val_loss: 0.1958\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1790 - val_loss: 0.1953\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1759 - val_loss: 0.1976\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1722 - val_loss: 0.1977\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1688 - val_loss: 0.2014\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1662 - val_loss: 0.2023\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1641 - val_loss: 0.2050\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1608 - val_loss: 0.2073\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1586 - val_loss: 0.2149\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1570 - val_loss: 0.2131\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1551 - val_loss: 0.2174\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 17s 117ms/step - loss: 0.1518 - val_loss: 0.2199\n",
      "Epoch 00018: early stopping\n",
      "accuracy: 0.9183132796114362, precision: 0.6076520483667208, recall: 0.3918305597579425, f1: 0.47643979057591623, specificity0.9734868348842031, roc_auc0.6826586973210728\n",
      "accuracy: 0.9183132796114362, precision: 0.6076520483667208, recall: 0.3918305597579425, f1: 0.47643979057591623, specificity0.9734868348842031, roc_auc0.6826586973210728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score1 = []\n",
    "for i in range(5):\n",
    "    model = Model_Build(text_num, vocab_size)\n",
    "\n",
    "    adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)\n",
    "\n",
    "\n",
    "    history = model.fit([train_text, train_rating, train_depth, train_structure, train_extremity, train_photo, train_style, train_reputation, train_experience, train_location], \n",
    "                train_fake, \n",
    "                batch_size = 2048, \n",
    "                epochs = 100,\n",
    "                callbacks=[es],\n",
    "                validation_split = 0.2)\n",
    "\n",
    "    prediction = model.predict([test_text, test_rating, test_depth, test_structure, test_extremity, test_photo, test_style, test_reputation, test_experience, test_location])\n",
    "\n",
    "    pred_class1 = np.where(prediction >= 0.5, 1 , 0)\n",
    "    \n",
    "    score1.append(get_clf_eval(test_fake, pred_class1))\n",
    "    print(score1)\n",
    "print(get_mean(score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89227ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN 없는거\n",
    "def Model_Build():\n",
    "    \n",
    "    tf.random.set_seed(215)\n",
    "    \n",
    "    \n",
    "    #linguistic\n",
    "    affect_input = Input(shape = (1,))\n",
    "    dense_affect = Dense(units=64, activation='relu', name='dense_affect')(affect_input)\n",
    "    motion_input = Input(shape = (1,))\n",
    "    dense_motion = Dense(units=64, activation='relu', name='motion')(motion_input)\n",
    "    cognitive_input = Input(shape = (1,))\n",
    "    dense_cognitive = Dense(units=64, activation='relu', name='dense_cognitive')(cognitive_input)\n",
    "    perception_input = Input(shape = (1,))\n",
    "    dense_perception = Dense(units=64, activation='relu', name='dense_perception')(perception_input)\n",
    "    social_input = Input(shape = (1,))\n",
    "    dense_social = Dense(units=64, activation='relu', name='dense_social')(social_input)\n",
    "    p_dis_input = Input(shape = (1,))\n",
    "    dense_p_dis = Dense(units=64, activation='relu', name='dense_p_dis')(p_dis_input)\n",
    "\n",
    "    linguistic_feature = Concatenate(name = 'Linguistic_Feature_Layer')([dense_affect, dense_motion, dense_cognitive, dense_perception, dense_social, dense_p_dis])\n",
    "    linguistic_feature1 = Dense(units=128, activation='relu', name='dense_linguistic')(linguistic_feature)\n",
    "\n",
    "    #review\n",
    "    rating_input = Input(shape = (1,))\n",
    "    dense_rating = Dense(units=64, activation='relu', name='dense_rating')(rating_input)\n",
    "    depth_input = Input(shape = (1,))\n",
    "    dense_depth = Dense(units=64, activation='relu', name='dense_depth')(depth_input)\n",
    "    structure_input = Input(shape = (1,))\n",
    "    dense_structure = Dense(units=64, activation='relu', name='dense_structure')(structure_input)\n",
    "    extremity_input = Input(shape = (1,))\n",
    "    dense_extremity = Dense(units=64, activation='relu', name='dense_extremity')(extremity_input)\n",
    "    photo_input = Input(shape = (1,))\n",
    "    dense_photo = Dense(units=64, activation='relu', name='dense_photo')(photo_input)\n",
    "    style_input = Input(shape = (1,))\n",
    "    dense_style = Dense(units=64, activation='relu', name='dense_style')(style_input)\n",
    "\n",
    "    review_feature = Concatenate(name = 'review_Feature_Layer')([dense_rating, dense_depth, dense_structure, dense_extremity, dense_photo, dense_style])\n",
    "    review_feature1 = Dense(units=128, activation='relu', name='dense_review')(review_feature)\n",
    "\n",
    "    #reviewer\n",
    "    \n",
    "    reputation_input = Input(shape = (1,))\n",
    "    dense_reputation = Dense(units=64, activation='relu', name='dense_reputation')(reputation_input)\n",
    "    experience_input = Input(shape = (1,))\n",
    "    dense_experience = Dense(units=64, activation='relu', name='dense_experience')(experience_input)\n",
    "    location_input = Input(shape = (1,))\n",
    "    dense_location = Dense(units=64, activation='relu', name='dense_location')(location_input)\n",
    "    \n",
    "    reviewer_feature = Concatenate(name = 'reviewer_Feature_Layer')([dense_reputation, dense_location, dense_experience])\n",
    "    reviewer_feature1 = Dense(units=128, activation='relu', name='dense_reviewer')(reviewer_feature)\n",
    "\n",
    "    all_feature = Concatenate(name = 'All_Feature_Layer')([reviewer_feature1, review_feature1, linguistic_feature1])\n",
    "    #all_feature1 = Dense(units=256, activation='relu', name='dense_all')(all_feature)\n",
    "    #all_feature2 = Dense(units=128, activation='relu', name='dense_all1')(all_feature)\n",
    "    all_feature3 = Dense(units=64, activation='relu', name='dense_all2')(all_feature)\n",
    "    all_feature4 = Dense(units=32, activation='relu', name='dense_all3')(all_feature3)\n",
    "    all_feature5 = Dense(units=16, activation='relu', name='dense_all4')(all_feature4)\n",
    "\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid', name = 'outputs')(all_feature5)  # (1,1) / h(8,1)초기화\n",
    "    model = Model(inputs=[affect_input, motion_input, cognitive_input, perception_input,social_input, p_dis_input, rating_input, depth_input, structure_input, extremity_input, photo_input, style_input, reputation_input, experience_input, location_input], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3439a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 16ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2120 - val_loss: 0.2114\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2073 - val_loss: 0.2057\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2037 - val_loss: 0.2029\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2012 - val_loss: 0.2025\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2003 - val_loss: 0.2043\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1999 - val_loss: 0.2033\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.2019\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1991 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.1995\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1980 - val_loss: 0.2053\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1990 - val_loss: 0.2006\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1977 - val_loss: 0.2012\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1996\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.2047\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1984\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1983\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1964 - val_loss: 0.1992\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1979\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1988\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1961 - val_loss: 0.1990\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1961 - val_loss: 0.1982\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1960 - val_loss: 0.1983\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1998\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1955 - val_loss: 0.1983\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1954 - val_loss: 0.2036\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1957 - val_loss: 0.1990\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1984\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1977\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1953 - val_loss: 0.1982\n",
      "Epoch 00035: early stopping\n",
      "[[0.9194723479412739, 0.6437112488928255, 0.3382986151518678, 0.4435120909298955, 0.980377330877959, 0.6593379730149135]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2148\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2113 - val_loss: 0.2087\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2048\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2038 - val_loss: 0.2027\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2024\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2045\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2029\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2010\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2024\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2004\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1986 - val_loss: 0.1993\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2005\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.1998\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1996\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1993\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1995\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.2051\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1986\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1997\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1987\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1989\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1982\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1986\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.2007\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1984\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1990\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2023\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1993\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1989\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1984\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9194723479412739, 0.6437112488928255, 0.3382986151518678, 0.4435120909298955, 0.980377330877959, 0.6593379730149135], [0.9192626117673033, 0.6393549792983221, 0.34144070755265915, 0.44515248065543916, 0.979816334743954, 0.6606285211483066]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2187 - val_loss: 0.2145\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2114 - val_loss: 0.2094\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2070 - val_loss: 0.2051\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2034 - val_loss: 0.2028\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2012 - val_loss: 0.2026\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2003 - val_loss: 0.2040\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2030\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2007\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2021\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2008\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.1993\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2020\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1979 - val_loss: 0.2005\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1974 - val_loss: 0.1991\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1999\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.2038\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1969 - val_loss: 0.1995\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1964 - val_loss: 0.1999\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1984\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1986\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1969 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1956 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.1994\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1958 - val_loss: 0.2014\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1982\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1997\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1954 - val_loss: 0.2034\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1991\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1988\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1979\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1984\n",
      "Epoch 00035: early stopping\n",
      "[[0.9194723479412739, 0.6437112488928255, 0.3382986151518678, 0.4435120909298955, 0.980377330877959, 0.6593379730149135], [0.9192626117673033, 0.6393549792983221, 0.34144070755265915, 0.44515248065543916, 0.979816334743954, 0.6606285211483066], [0.9189535268793465, 0.6339113680154143, 0.3445827999534505, 0.4464716525934861, 0.9791455784967743, 0.6618641892251125]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2113 - val_loss: 0.2085\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2061\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2040 - val_loss: 0.2032\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2057\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2030\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1996 - val_loss: 0.2011\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1991 - val_loss: 0.2031\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.2013\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.1995\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1978 - val_loss: 0.2009\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.2004\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1986\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1996\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1975 - val_loss: 0.2049\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1987\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1984\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.2002\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1987\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.2007\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1984\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1959 - val_loss: 0.2009\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1981\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1997\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2045\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1985\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1984\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1978\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1982\n",
      "Epoch 00035: early stopping\n",
      "[[0.9194723479412739, 0.6437112488928255, 0.3382986151518678, 0.4435120909298955, 0.980377330877959, 0.6593379730149135], [0.9192626117673033, 0.6393549792983221, 0.34144070755265915, 0.44515248065543916, 0.979816334743954, 0.6606285211483066], [0.9189535268793465, 0.6339113680154143, 0.3445827999534505, 0.4464716525934861, 0.9791455784967743, 0.6618641892251125], [0.9192846892593002, 0.6391483814903324, 0.34237169789363436, 0.4458926947559867, 0.9797431613351708, 0.6610574296144026]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2187 - val_loss: 0.2148\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2106 - val_loss: 0.2085\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2057\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2040 - val_loss: 0.2032\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2015 - val_loss: 0.2026\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2042\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2027\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1999 - val_loss: 0.2001\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1987 - val_loss: 0.2025\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2004\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1983 - val_loss: 0.1996\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1990 - val_loss: 0.2026\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2002\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1977 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1997\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1972 - val_loss: 0.2041\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1968 - val_loss: 0.1988\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1965 - val_loss: 0.1988\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1995\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1988\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1990\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1983\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1984\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2017\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1956 - val_loss: 0.1981\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1994\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2056\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1998\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1992\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1951 - val_loss: 0.1977\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1956 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9194723479412739, 0.6437112488928255, 0.3382986151518678, 0.4435120909298955, 0.980377330877959, 0.6593379730149135], [0.9192626117673033, 0.6393549792983221, 0.34144070755265915, 0.44515248065543916, 0.979816334743954, 0.6606285211483066], [0.9189535268793465, 0.6339113680154143, 0.3445827999534505, 0.4464716525934861, 0.9791455784967743, 0.6618641892251125], [0.9192846892593002, 0.6391483814903324, 0.34237169789363436, 0.4458926947559867, 0.9797431613351708, 0.6610574296144026], [0.9191963792913125, 0.6383996520982822, 0.3416734551379029, 0.44511825348696177, 0.9797187701989097, 0.6606961126684064]]\n",
      "accuracy: 0.9191963792913125, precision: 0.6383996520982822, recall: 0.3416734551379029, f1: 0.44511825348696177, specificity0.9797187701989097, roc_auc0.6606961126684064\n",
      "accuracy: 0.9191963792913125, precision: 0.6383996520982822, recall: 0.3416734551379029, f1: 0.44511825348696177, specificity0.9797187701989097, roc_auc0.6606961126684064\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 14ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2148\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2112 - val_loss: 0.2088\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2072 - val_loss: 0.2051\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2033 - val_loss: 0.2045\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2048\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2029\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2019\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.1995\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.2004\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.2005\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.2030\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1979 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1993\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2071\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1984\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1986\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1997\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1986\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.2005\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.1988\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1978\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1987\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1965 - val_loss: 0.1995\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2005\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1987\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2004\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2023\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1986\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1956 - val_loss: 0.1986\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1950 - val_loss: 0.1979\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1987\n",
      "Epoch 00035: early stopping\n",
      "[[0.9190970305773264, 0.636265631737818, 0.3434190620272315, 0.44607361499508735, 0.9794260765637767, 0.661422569295504]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2150\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2108 - val_loss: 0.2086\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2035 - val_loss: 0.2028\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2025\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2050\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1999 - val_loss: 0.2025\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2005\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1988 - val_loss: 0.2022\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1985 - val_loss: 0.2003\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1981 - val_loss: 0.1993\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2008\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1979 - val_loss: 0.2000\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1975 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1989\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1998\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2041\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1993\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1965 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 14ms/step - loss: 0.1964 - val_loss: 0.2003\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1963 - val_loss: 0.1988\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1959 - val_loss: 0.2012\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1989 - val_loss: 0.1990\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1959 - val_loss: 0.1981\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1963 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1961 - val_loss: 0.1988\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2007\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1983\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1956 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1954 - val_loss: 0.2022\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1985\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1989\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1980\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1984\n",
      "Epoch 36/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1990\n",
      "Epoch 37/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1945 - val_loss: 0.1983\n",
      "Epoch 38/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1947 - val_loss: 0.1976\n",
      "Epoch 39/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1947 - val_loss: 0.1976\n",
      "Epoch 40/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1944 - val_loss: 0.1978\n",
      "Epoch 41/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1949 - val_loss: 0.1990\n",
      "Epoch 42/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1946 - val_loss: 0.1985\n",
      "Epoch 43/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1945 - val_loss: 0.1981\n",
      "Epoch 44/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1945 - val_loss: 0.1975\n",
      "Epoch 45/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1943 - val_loss: 0.1989\n",
      "Epoch 46/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1947 - val_loss: 0.2001\n",
      "Epoch 47/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1947 - val_loss: 0.1985\n",
      "Epoch 48/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1943 - val_loss: 0.1986\n",
      "Epoch 49/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1942 - val_loss: 0.1979\n",
      "Epoch 50/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1941 - val_loss: 0.1973\n",
      "Epoch 51/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1937 - val_loss: 0.1974\n",
      "Epoch 52/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1939 - val_loss: 0.1998\n",
      "Epoch 53/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1939 - val_loss: 0.1984\n",
      "Epoch 54/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1937 - val_loss: 0.1973\n",
      "Epoch 55/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1934 - val_loss: 0.1981\n",
      "Epoch 56/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1937 - val_loss: 0.1977\n",
      "Epoch 57/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1935 - val_loss: 0.1974\n",
      "Epoch 58/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1933 - val_loss: 0.1979\n",
      "Epoch 59/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1938 - val_loss: 0.1990\n",
      "Epoch 60/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1935 - val_loss: 0.1988\n",
      "Epoch 00060: early stopping\n",
      "[[0.9190970305773264, 0.636265631737818, 0.3434190620272315, 0.44607361499508735, 0.9794260765637767, 0.661422569295504], [0.9189976818633403, 0.7096558636819245, 0.2471779355289189, 0.366649404453651, 0.9894020512945596, 0.6182899934117393]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2148\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2108 - val_loss: 0.2088\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2072 - val_loss: 0.2047\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2027\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2025\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2035\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2031\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1989 - val_loss: 0.2019\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2008\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1988 - val_loss: 0.1992\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1978 - val_loss: 0.1996\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.2002\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.1998\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.1991\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1998\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.2063\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1990\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1991\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1986\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1996\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1995\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1957 - val_loss: 0.1978\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1968 - val_loss: 0.1989\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1967 - val_loss: 0.1988\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.2005\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1985\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1997\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2052\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1999\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1961 - val_loss: 0.1990\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1954 - val_loss: 0.1981\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1987\n",
      "Epoch 00035: early stopping\n",
      "[[0.9190970305773264, 0.636265631737818, 0.3434190620272315, 0.44607361499508735, 0.9794260765637767, 0.661422569295504], [0.9189976818633403, 0.7096558636819245, 0.2471779355289189, 0.366649404453651, 0.9894020512945596, 0.6182899934117393], [0.9190197593553372, 0.6332414670341319, 0.34760851856161995, 0.4488354620586026, 0.9789016671341635, 0.6632550928478917]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2188 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2107 - val_loss: 0.2086\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2065 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2030\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2015 - val_loss: 0.2025\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2042\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2000 - val_loss: 0.2023\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2012\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1988 - val_loss: 0.2025\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1981 - val_loss: 0.2006\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.1993\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1980 - val_loss: 0.2023\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.1998\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1974 - val_loss: 0.1990\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1996\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2055\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1988\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1964 - val_loss: 0.1986\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1962 - val_loss: 0.1993\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1991\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1992\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1988\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1955 - val_loss: 0.1976\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1981\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1965 - val_loss: 0.1992\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1959 - val_loss: 0.2010\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1983\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.2037\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1989\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1949 - val_loss: 0.1984\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1981\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1987\n",
      "Epoch 00035: early stopping\n",
      "[[0.9190970305773264, 0.636265631737818, 0.3434190620272315, 0.44607361499508735, 0.9794260765637767, 0.661422569295504], [0.9189976818633403, 0.7096558636819245, 0.2471779355289189, 0.366649404453651, 0.9894020512945596, 0.6182899934117393], [0.9190197593553372, 0.6332414670341319, 0.34760851856161995, 0.4488354620586026, 0.9789016671341635, 0.6632550928478917], [0.918964565625345, 0.6323467230443974, 0.34807401373210756, 0.44899797342940784, 0.9787919070209886, 0.6634329603765481]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2145\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2106 - val_loss: 0.2085\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2060 - val_loss: 0.2059\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2039 - val_loss: 0.2031\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2029\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2004 - val_loss: 0.2020\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1997 - val_loss: 0.2039\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2016\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1991 - val_loss: 0.2023\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2010\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1987 - val_loss: 0.1994\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1983 - val_loss: 0.2009\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1978 - val_loss: 0.2008\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1970 - val_loss: 0.1995\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.2049\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1986\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1985\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1963 - val_loss: 0.1995\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1987\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1960 - val_loss: 0.1987\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1995\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1957 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1987\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.2005\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1956 - val_loss: 0.1982\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.2000\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1954 - val_loss: 0.2029\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1954 - val_loss: 0.2003\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1952 - val_loss: 0.1989\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1950 - val_loss: 0.1982\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1984\n",
      "Epoch 00035: early stopping\n",
      "[[0.9190970305773264, 0.636265631737818, 0.3434190620272315, 0.44607361499508735, 0.9794260765637767, 0.661422569295504], [0.9189976818633403, 0.7096558636819245, 0.2471779355289189, 0.366649404453651, 0.9894020512945596, 0.6182899934117393], [0.9190197593553372, 0.6332414670341319, 0.34760851856161995, 0.4488354620586026, 0.9789016671341635, 0.6632550928478917], [0.918964565625345, 0.6323467230443974, 0.34807401373210756, 0.44899797342940784, 0.9787919070209886, 0.6634329603765481], [0.9194944254332708, 0.6438689685701637, 0.3385313627371116, 0.4437495233010449, 0.980377330877959, 0.6594543468075353]]\n",
      "accuracy: 0.9194944254332708, precision: 0.6438689685701637, recall: 0.3385313627371116, f1: 0.4437495233010449, specificity0.980377330877959, roc_auc0.6594543468075353\n",
      "accuracy: 0.9194944254332708, precision: 0.6438689685701637, recall: 0.3385313627371116, f1: 0.4437495233010449, specificity0.980377330877959, roc_auc0.6594543468075353\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2123 - val_loss: 0.2093\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2075 - val_loss: 0.2061\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2036 - val_loss: 0.2030\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2015 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2002 - val_loss: 0.2018\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1995 - val_loss: 0.2039\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2002 - val_loss: 0.2018\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1990 - val_loss: 0.2029\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2009\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1986 - val_loss: 0.1999\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.1997\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1997\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.1993\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1998\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.2055\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1990\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1994\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1990\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.2008\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1988\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1988\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.2008\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1985\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1998\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2041\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1984\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1984\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1952 - val_loss: 0.1979\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9193509217352909, 0.6370023419203747, 0.3481903875247294, 0.4502633559066968, 0.9792065563374269, 0.6636984719310781]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2106 - val_loss: 0.2085\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2065 - val_loss: 0.2058\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2038 - val_loss: 0.2030\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2024\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1996 - val_loss: 0.2035\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2013\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1989 - val_loss: 0.2021\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.2012\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.1995\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2043\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1989 - val_loss: 0.2007\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1977 - val_loss: 0.1997\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.1990\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1999\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2054\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1987\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1986\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.2016\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1988\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.2011\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1978\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1989\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1987\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2002\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1980\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1995\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.2040\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1996\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1996\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1984\n",
      "Epoch 00035: early stopping\n",
      "[[0.9193509217352909, 0.6370023419203747, 0.3481903875247294, 0.4502633559066968, 0.9792065563374269, 0.6636984719310781], [0.9189535268793465, 0.6339687299207539, 0.34446642616082856, 0.4463881767455889, 0.9791577740649048, 0.6618121001128667]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 14ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2107 - val_loss: 0.2089\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2061 - val_loss: 0.2065\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2032\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2024\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2037\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2029\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2006 - val_loss: 0.2010\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1987 - val_loss: 0.2027\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1980 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.1995\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1978 - val_loss: 0.2002\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.2005\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1976 - val_loss: 0.1993\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.1989\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1996\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.2040\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1991\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1996\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1989\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1988\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1990\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1987\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.2006\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1960 - val_loss: 0.1981\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2054\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2007\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1990\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1950 - val_loss: 0.1980\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1986\n",
      "Epoch 00035: early stopping\n",
      "[[0.9193509217352909, 0.6370023419203747, 0.3481903875247294, 0.4502633559066968, 0.9792065563374269, 0.6636984719310781], [0.9189535268793465, 0.6339687299207539, 0.34446642616082856, 0.4463881767455889, 0.9791577740649048, 0.6618121001128667], [0.918964565625345, 0.6352051835853132, 0.34225532410101245, 0.4448309763291235, 0.9794016854275156, 0.660828504764264]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 14ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2109 - val_loss: 0.2088\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2064\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2037 - val_loss: 0.2030\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2030\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2048\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2031\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2024\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1990 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.1996\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.2022\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2006\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.1990\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1989\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1998\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2046\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1988\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1988\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.2009\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.1986\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1984\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1987\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1982\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1984\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.2002\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1979\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1992\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.2055\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1958 - val_loss: 0.2008\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1950 - val_loss: 0.1980\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9193509217352909, 0.6370023419203747, 0.3481903875247294, 0.4502633559066968, 0.9792065563374269, 0.6636984719310781], [0.9189535268793465, 0.6339687299207539, 0.34446642616082856, 0.4463881767455889, 0.9791577740649048, 0.6618121001128667], [0.918964565625345, 0.6352051835853132, 0.34225532410101245, 0.4448309763291235, 0.9794016854275156, 0.660828504764264], [0.918964565625345, 0.6352051835853132, 0.34225532410101245, 0.4448309763291235, 0.9794016854275156, 0.660828504764264]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2109 - val_loss: 0.2087\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2072 - val_loss: 0.2051\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2030 - val_loss: 0.2030\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2048\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2007 - val_loss: 0.2040\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2040\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1997 - val_loss: 0.2008\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1989 - val_loss: 0.2022\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2006\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.1993\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1983 - val_loss: 0.2022\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1979 - val_loss: 0.2001\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1994\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1993\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.2045\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.1988\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1968 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.2007\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1989\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.2003\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1989\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1976\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1985\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2010\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1982\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.2038\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1981\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1991\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1949 - val_loss: 0.1976\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1986\n",
      "Epoch 00035: early stopping\n",
      "[[0.9193509217352909, 0.6370023419203747, 0.3481903875247294, 0.4502633559066968, 0.9792065563374269, 0.6636984719310781], [0.9189535268793465, 0.6339687299207539, 0.34446642616082856, 0.4463881767455889, 0.9791577740649048, 0.6618121001128667], [0.918964565625345, 0.6352051835853132, 0.34225532410101245, 0.4448309763291235, 0.9794016854275156, 0.660828504764264], [0.918964565625345, 0.6352051835853132, 0.34225532410101245, 0.4448309763291235, 0.9794016854275156, 0.660828504764264], [0.9188541781653604, 0.6400541271989174, 0.3302688234609566, 0.4357104475320488, 0.980535873263656, 0.6554023483623063]]\n",
      "accuracy: 0.9188541781653604, precision: 0.6400541271989174, recall: 0.3302688234609566, f1: 0.4357104475320488, specificity0.980535873263656, roc_auc0.6554023483623063\n",
      "accuracy: 0.9188541781653604, precision: 0.6400541271989174, recall: 0.3302688234609566, f1: 0.4357104475320488, specificity0.980535873263656, roc_auc0.6554023483623063\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2117 - val_loss: 0.2092\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2062 - val_loss: 0.2063\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2031\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2016 - val_loss: 0.2026\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2004 - val_loss: 0.2031\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2033\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2004\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2023\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1990 - val_loss: 0.2006\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.1992\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2023\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2005\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1977 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1976 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1971 - val_loss: 0.1999\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2036\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1985\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1988\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1998\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1991\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.1992\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1988\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1980\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.1987\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1993\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.2002\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1982\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.2000\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2048\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1957 - val_loss: 0.1993\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1987\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1952 - val_loss: 0.1980\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1988\n",
      "Epoch 00035: early stopping\n",
      "[[0.9188652169113589, 0.6320935175345377, 0.3460956592575352, 0.4472853060610618, 0.9788894715660329, 0.6624925654117841]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2150\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2109 - val_loss: 0.2085\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2062 - val_loss: 0.2048\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2030\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2025\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2005 - val_loss: 0.2009\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2002 - val_loss: 0.2026\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2000\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.2017\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1980 - val_loss: 0.2008\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1990 - val_loss: 0.1994\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1979 - val_loss: 0.2024\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.2004\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1993\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1970 - val_loss: 0.1995\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.2052\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1985\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1987\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1964 - val_loss: 0.2003\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1992\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1959 - val_loss: 0.1988\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1991\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1956 - val_loss: 0.1978\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1982\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1987\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1959 - val_loss: 0.2012\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1984\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1995\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2029\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1984\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1986\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1979\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1987\n",
      "Epoch 00035: early stopping\n",
      "[[0.9188652169113589, 0.6320935175345377, 0.3460956592575352, 0.4472853060610618, 0.9788894715660329, 0.6624925654117841], [0.9189866431173419, 0.634780739466896, 0.34365180961247527, 0.44590411476028685, 0.9792797297462102, 0.6614657696793428]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2150\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2112 - val_loss: 0.2086\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2071 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2037 - val_loss: 0.2027\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2013 - val_loss: 0.2029\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2039\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2030\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2002 - val_loss: 0.2008\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1989 - val_loss: 0.2027\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1986 - val_loss: 0.1992\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2000\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1977 - val_loss: 0.2003\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1996\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.1995\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2066\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1990\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1988\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1993\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1988\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1960 - val_loss: 0.1987\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1983\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1976\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1985\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1983\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.2008\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1980\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.2029\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.2016\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1988\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1983\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9188652169113589, 0.6320935175345377, 0.3460956592575352, 0.4472853060610618, 0.9788894715660329, 0.6624925654117841], [0.9189866431173419, 0.634780739466896, 0.34365180961247527, 0.44590411476028685, 0.9792797297462102, 0.6614657696793428], [0.9191632630533172, 0.6392543859649122, 0.33922960549284303, 0.44324488709800053, 0.9799382904252595, 0.6595839479590512]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2188 - val_loss: 0.2148\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2107 - val_loss: 0.2088\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2076 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2031 - val_loss: 0.2028\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2011 - val_loss: 0.2024\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2002 - val_loss: 0.2044\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2032\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2004\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2026\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2005\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.1996\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.2060\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1993 - val_loss: 0.2007\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.1989\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1992\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.2058\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1992\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1986\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1994\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1989\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1986\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1996\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1976\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1983\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1991\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.2014\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1954 - val_loss: 0.1981\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1996\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1953 - val_loss: 0.2048\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2003\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1989\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1950 - val_loss: 0.1983\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1983\n",
      "Epoch 00035: early stopping\n",
      "[[0.9188652169113589, 0.6320935175345377, 0.3460956592575352, 0.4472853060610618, 0.9788894715660329, 0.6624925654117841], [0.9189866431173419, 0.634780739466896, 0.34365180961247527, 0.44590411476028685, 0.9792797297462102, 0.6614657696793428], [0.9191632630533172, 0.6392543859649122, 0.33922960549284303, 0.44324488709800053, 0.9799382904252595, 0.6595839479590512], [0.9190970305773264, 0.6403197158081705, 0.33562201792156404, 0.44040619989310525, 0.980243179628523, 0.6579325987750436]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2144\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2105 - val_loss: 0.2086\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2056\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2036 - val_loss: 0.2035\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2015 - val_loss: 0.2028\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2037\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2029\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2003 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.2021\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1980 - val_loss: 0.2010\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.1998\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.2011\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.2007\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2004\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1971 - val_loss: 0.1996\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.2057\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1989\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1966 - val_loss: 0.1984\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1996\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1988\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1986\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1986\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1980\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1992\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.2007\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1982\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1997\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.2048\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.2001\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1985\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1979\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1987\n",
      "Epoch 00035: early stopping\n",
      "[[0.9188652169113589, 0.6320935175345377, 0.3460956592575352, 0.4472853060610618, 0.9788894715660329, 0.6624925654117841], [0.9189866431173419, 0.634780739466896, 0.34365180961247527, 0.44590411476028685, 0.9792797297462102, 0.6614657696793428], [0.9191632630533172, 0.6392543859649122, 0.33922960549284303, 0.44324488709800053, 0.9799382904252595, 0.6595839479590512], [0.9190970305773264, 0.6403197158081705, 0.33562201792156404, 0.44040619989310525, 0.980243179628523, 0.6579325987750436], [0.9187437907053758, 0.6334488734835355, 0.3402769696264401, 0.44272844272844275, 0.979365098723124, 0.659821034174782]]\n",
      "accuracy: 0.9187437907053758, precision: 0.6334488734835355, recall: 0.3402769696264401, f1: 0.44272844272844275, specificity0.979365098723124, roc_auc0.659821034174782\n",
      "accuracy: 0.9187437907053758, precision: 0.6334488734835355, recall: 0.3402769696264401, f1: 0.44272844272844275, specificity0.979365098723124, roc_auc0.659821034174782\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2247\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2148\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2114 - val_loss: 0.2090\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2074 - val_loss: 0.2047\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.2029 - val_loss: 0.2026\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2017 - val_loss: 0.2044\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2005 - val_loss: 0.2020\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1996 - val_loss: 0.2035\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1990 - val_loss: 0.2024\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.1994\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1986 - val_loss: 0.1997\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.2006\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1976 - val_loss: 0.2009\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1993\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2040\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1985\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1988\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.2006\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1989\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.2009\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.1988\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1978\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1990\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.2003\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1982\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1991\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2017\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2015\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1992\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1981\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1989\n",
      "Epoch 00035: early stopping\n",
      "[[0.9189204106413511, 0.6330490405117271, 0.3455137902944257, 0.44703756681472556, 0.9790114272473384, 0.662262608770882]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2149\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.2108 - val_loss: 0.2087\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2064 - val_loss: 0.2064\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2040 - val_loss: 0.2031\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2027\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2009 - val_loss: 0.2039\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2041\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2008\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1991 - val_loss: 0.2020\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2015\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.1993\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2030\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1985 - val_loss: 0.2006\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1992\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1989\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1993\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1971 - val_loss: 0.2060\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1969 - val_loss: 0.1990\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1968 - val_loss: 0.1991\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.2003\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1985\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1992\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1993\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1978\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1985\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1987\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.2008\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1964 - val_loss: 0.1983\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1993\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2029\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1987\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1953 - val_loss: 0.1989\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.1983\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1957 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9189204106413511, 0.6330490405117271, 0.3455137902944257, 0.44703756681472556, 0.9790114272473384, 0.662262608770882], [0.9192736505133017, 0.6424121050289274, 0.3359711392994298, 0.44120119202261787, 0.98040172201422, 0.6581864306568249]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2147\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2106 - val_loss: 0.2088\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2058\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2037 - val_loss: 0.2033\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2014 - val_loss: 0.2037\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2008 - val_loss: 0.2046\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2000 - val_loss: 0.2019\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2005\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.2021\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2005\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1987 - val_loss: 0.1995\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.2021\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.2002\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1978 - val_loss: 0.1997\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1971 - val_loss: 0.1995\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1974 - val_loss: 0.2033\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1980 - val_loss: 0.1996\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1965 - val_loss: 0.1986\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.2000\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1987\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1985\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1965 - val_loss: 0.1987\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1976\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1983\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1990\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1959 - val_loss: 0.2006\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1981\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1998\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2032\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1986\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1985\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1950 - val_loss: 0.1979\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1951 - val_loss: 0.1988\n",
      "Epoch 00035: early stopping\n",
      "[[0.9189204106413511, 0.6330490405117271, 0.3455137902944257, 0.44703756681472556, 0.9790114272473384, 0.662262608770882], [0.9192736505133017, 0.6424121050289274, 0.3359711392994298, 0.44120119202261787, 0.98040172201422, 0.6581864306568249], [0.919207418037311, 0.6395705521472392, 0.33969510066333064, 0.4437181728357528, 0.9799382904252595, 0.6598166955442951]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 17ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2187 - val_loss: 0.2145\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2115 - val_loss: 0.2090\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2063 - val_loss: 0.2063\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2037 - val_loss: 0.2029\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2013 - val_loss: 0.2027\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.2004 - val_loss: 0.2045\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1998 - val_loss: 0.2041\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2006\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1987 - val_loss: 0.2029\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2011\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1983 - val_loss: 0.1998\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1984 - val_loss: 0.1998\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 11ms/step - loss: 0.1974 - val_loss: 0.2005\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1988\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1987\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1996\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1973 - val_loss: 0.2061\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1970 - val_loss: 0.1987\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1989\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1967 - val_loss: 0.2008\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1988\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1984\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1962 - val_loss: 0.1988\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1976\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1984\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1985\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.2013\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1979\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1999\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.2034\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1959 - val_loss: 0.1988\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1988\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1981\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1985\n",
      "Epoch 00035: early stopping\n",
      "[[0.9189204106413511, 0.6330490405117271, 0.3455137902944257, 0.44703756681472556, 0.9790114272473384, 0.662262608770882], [0.9192736505133017, 0.6424121050289274, 0.3359711392994298, 0.44120119202261787, 0.98040172201422, 0.6581864306568249], [0.919207418037311, 0.6395705521472392, 0.33969510066333064, 0.4437181728357528, 0.9799382904252595, 0.6598166955442951], [0.9193067667512971, 0.6366347177848776, 0.3478412661468637, 0.4498795906080674, 0.9791943607692964, 0.66351781345808]]\n",
      "Epoch 1/100\n",
      "142/142 [==============================] - 3s 13ms/step - loss: 0.2407 - val_loss: 0.2248\n",
      "Epoch 2/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2188 - val_loss: 0.2156\n",
      "Epoch 3/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2114 - val_loss: 0.2085\n",
      "Epoch 4/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2071 - val_loss: 0.2049\n",
      "Epoch 5/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2035 - val_loss: 0.2026\n",
      "Epoch 6/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2015 - val_loss: 0.2025\n",
      "Epoch 7/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2004 - val_loss: 0.2032\n",
      "Epoch 8/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1999 - val_loss: 0.2021\n",
      "Epoch 9/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.2001 - val_loss: 0.2010\n",
      "Epoch 10/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.2028\n",
      "Epoch 11/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2007\n",
      "Epoch 12/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1988 - val_loss: 0.1994\n",
      "Epoch 13/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1979 - val_loss: 0.2006\n",
      "Epoch 14/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1981 - val_loss: 0.2003\n",
      "Epoch 15/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1975 - val_loss: 0.1990\n",
      "Epoch 16/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1972 - val_loss: 0.1988\n",
      "Epoch 17/100\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 0.1969 - val_loss: 0.1992\n",
      "Epoch 18/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1977 - val_loss: 0.2046\n",
      "Epoch 19/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1968 - val_loss: 0.1986\n",
      "Epoch 20/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1965 - val_loss: 0.1986\n",
      "Epoch 21/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1995\n",
      "Epoch 22/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1961 - val_loss: 0.1986\n",
      "Epoch 23/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1987\n",
      "Epoch 24/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1964 - val_loss: 0.1991\n",
      "Epoch 25/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1954 - val_loss: 0.1977\n",
      "Epoch 26/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.1983\n",
      "Epoch 27/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1960 - val_loss: 0.1984\n",
      "Epoch 28/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1958 - val_loss: 0.2001\n",
      "Epoch 29/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1963 - val_loss: 0.1983\n",
      "Epoch 30/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1956 - val_loss: 0.1991\n",
      "Epoch 31/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1953 - val_loss: 0.2020\n",
      "Epoch 32/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.2006\n",
      "Epoch 33/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1988\n",
      "Epoch 34/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1952 - val_loss: 0.1984\n",
      "Epoch 35/100\n",
      "142/142 [==============================] - 2s 12ms/step - loss: 0.1955 - val_loss: 0.1986\n",
      "Epoch 00035: early stopping\n",
      "[[0.9189204106413511, 0.6330490405117271, 0.3455137902944257, 0.44703756681472556, 0.9790114272473384, 0.662262608770882], [0.9192736505133017, 0.6424121050289274, 0.3359711392994298, 0.44120119202261787, 0.98040172201422, 0.6581864306568249], [0.919207418037311, 0.6395705521472392, 0.33969510066333064, 0.4437181728357528, 0.9799382904252595, 0.6598166955442951], [0.9193067667512971, 0.6366347177848776, 0.3478412661468637, 0.4498795906080674, 0.9791943607692964, 0.66351781345808], [0.9192405342753063, 0.6413548815585566, 0.3371348772256488, 0.44195270785659796, 0.980243179628523, 0.658689028427086]]\n",
      "accuracy: 0.9192405342753063, precision: 0.6413548815585566, recall: 0.3371348772256488, f1: 0.44195270785659796, specificity0.980243179628523, roc_auc0.658689028427086\n",
      "accuracy: 0.9192405342753063, precision: 0.6413548815585566, recall: 0.3371348772256488, f1: 0.44195270785659796, specificity0.980243179628523, roc_auc0.658689028427086\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score = []\n",
    "for i in range(5):\n",
    "    model = Model_Build()\n",
    "\n",
    "    adam = Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 10)\n",
    "\n",
    "\n",
    "    history = model.fit([train_affect, train_motion, train_cognition, train_perception, train_social, train_p_dis, train_rating, train_depth, train_structure, train_extremity, train_photo, train_style, train_reputation, train_experience, train_location], \n",
    "                train_fake, \n",
    "                batch_size = 2048, \n",
    "                epochs = 100,\n",
    "                callbacks=[es],\n",
    "                validation_split = 0.2)\n",
    "\n",
    "    prediction = model.predict([test_affect, test_motion, test_cognition, test_perception, test_social, test_p_dis, test_rating, test_depth, test_structure, test_extremity, test_photo, test_style, test_reputation, test_experience, test_location])\n",
    "\n",
    "    pred_class = np.where(prediction >= 0.5, 1 , 0)\n",
    "    \n",
    "    score.append(get_clf_eval(test_fake, pred_class))\n",
    "    print(score)\n",
    "\n",
    "print(get_mean(score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "koo",
   "language": "python",
   "name": "koo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
